{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def calculate_statistics(result_path):\n",
    "    \n",
    "    total_rouge_scores = defaultdict(float)\n",
    "    count_rouge_scores = defaultdict(int)\n",
    "\n",
    "    total_bert_scores = defaultdict(float)\n",
    "    count_bert_scores = defaultdict(int)\n",
    "    \n",
    "    missing_count = 0\n",
    "\n",
    "    # get all result files\n",
    "    result_files = os.listdir(\"../../data/process_data\")\n",
    "    \n",
    "    for result_file in result_files:\n",
    "        with open(f\"../../data/process_data/{result_file}/{result_path}\", 'r', encoding='utf-8') as f:\n",
    "            report_list = json.load(f)\n",
    "            \n",
    "            for report in report_list:\n",
    "                # count \"Missing\" in the report\n",
    "                for section, content in report.items():\n",
    "                    if isinstance(content, str) and \"Missing\" in content:\n",
    "                        missing_count += 1\n",
    "\n",
    "                # calculate ROUGE scores\n",
    "                rouge = report.get(\"rouge\", {})\n",
    "                for rouge_metric in [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"]:\n",
    "                    for metric_type in [\"Precision\", \"Recall\", \"F1\"]:\n",
    "                        key = f\"{rouge_metric} {metric_type}\"\n",
    "                        value = rouge.get(key, 0)\n",
    "                        total_rouge_scores[key] += float(value)\n",
    "                        count_rouge_scores[key] += 1\n",
    "                \n",
    "                # calculate BERT scores\n",
    "                bert = report.get(\"bertscore\", {})\n",
    "                for rouge_metric in [\"BERTScore Precision\", \"BERTScore Recall\", \"BERTScore F1\"]:\n",
    "                    key = f\"{rouge_metric}\"\n",
    "                    value = bert.get(key, 0)\n",
    "                    total_bert_scores[key] += float(value)\n",
    "                    count_bert_scores[key] += 1\n",
    "\n",
    "    \n",
    "    # average ROUGE scores\n",
    "    average_rouge_scores = {key: (total_rouge_scores[key] / count_rouge_scores[key]) \n",
    "                            if count_rouge_scores[key] > 0 else 0 \n",
    "                            for key in total_rouge_scores}\n",
    "    \n",
    "    # average BERT scores\n",
    "    average_bert_scores = {key: (total_bert_scores[key] / count_bert_scores[key]) \n",
    "                            if count_bert_scores[key] > 0 else 0 \n",
    "                            for key in total_bert_scores}\n",
    "\n",
    "    # output results\n",
    "    print(\"\\n=== Average ROUGE Scores ===\")\n",
    "    for key, avg in average_rouge_scores.items():\n",
    "        print(f\"{key}: {avg:.4f}\")\n",
    "\n",
    "    print(\"\\n=== Average BERT Scores ===\")\n",
    "    for key, avg in average_bert_scores.items():\n",
    "        print(f\"{key}: {avg:.4f}\")\n",
    "\n",
    "    print(f\"\\nTotal 'Missing' count: {missing_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Average ROUGE Scores ===\n",
      "ROUGE-1 Precision: 0.3556\n",
      "ROUGE-1 Recall: 0.1621\n",
      "ROUGE-1 F1: 0.2041\n",
      "ROUGE-2 Precision: 0.0637\n",
      "ROUGE-2 Recall: 0.0297\n",
      "ROUGE-2 F1: 0.0368\n",
      "ROUGE-L Precision: 0.2222\n",
      "ROUGE-L Recall: 0.0980\n",
      "ROUGE-L F1: 0.1241\n",
      "\n",
      "=== Average BERT Scores ===\n",
      "BERTScore Precision: 0.6422\n",
      "BERTScore Recall: 0.5668\n",
      "BERTScore F1: 0.6000\n",
      "\n",
      "Total 'Missing' count: 0\n"
     ]
    }
   ],
   "source": [
    "calculate_statistics(\"AgenticIR_result/eval_self_reflection.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Average ROUGE Scores ===\n",
      "ROUGE-1 Precision: 0.4349\n",
      "ROUGE-1 Recall: 0.1771\n",
      "ROUGE-1 F1: 0.2353\n",
      "ROUGE-2 Precision: 0.0987\n",
      "ROUGE-2 Recall: 0.0392\n",
      "ROUGE-2 F1: 0.0522\n",
      "ROUGE-L Precision: 0.2824\n",
      "ROUGE-L Recall: 0.1123\n",
      "ROUGE-L F1: 0.1503\n",
      "\n",
      "=== Average BERT Scores ===\n",
      "BERTScore Precision: 0.6879\n",
      "BERTScore Recall: 0.5963\n",
      "BERTScore F1: 0.6376\n",
      "\n",
      "Total 'Missing' count: 0\n"
     ]
    }
   ],
   "source": [
    "calculate_statistics(\"AgenticIR_result/eval.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
