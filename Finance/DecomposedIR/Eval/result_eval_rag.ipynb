{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def calculate_statistics(result_path, company):\n",
    "    # Initialize accumulators and counters\n",
    "    total_scores1 = {\n",
    "        \"Financial takeaways\": 0,\n",
    "        \"Financial context\": 0,\n",
    "        \"Reasoning correctness\": 0,\n",
    "        \"Management expectation\": 0\n",
    "    }\n",
    "    count_scores1 = {\n",
    "        \"Financial takeaways\": 0,\n",
    "        \"Financial context\": 0,\n",
    "        \"Reasoning correctness\": 0,\n",
    "        \"Management expectation\": 0\n",
    "    }\n",
    "    total_scores2 = {\n",
    "        \"Financial takeaways\": 0,\n",
    "        \"Financial context\": 0,\n",
    "        \"Reasoning correctness\": 0,\n",
    "        \"Management expectation\": 0\n",
    "    }\n",
    "    count_scores2 = {\n",
    "        \"Financial takeaways\": 0,\n",
    "        \"Financial context\": 0,\n",
    "        \"Reasoning correctness\": 0,\n",
    "        \"Management expectation\": 0\n",
    "    }\n",
    "    missing_count = 0\n",
    "\n",
    "    # List result files\n",
    "    result_files = os.listdir(f\"../{result_path}/{company}\")\n",
    "    \n",
    "    for result_file in result_files:\n",
    "        with open(f\"../{result_path}/{company}/{result_file}\", 'r', encoding='utf-8') as f:\n",
    "            report_list = json.load(f)\n",
    "            \n",
    "            for report in report_list:\n",
    "                # Extract and check sections with possible \"Missing\"\n",
    "                for key, section in report.items():\n",
    "                    if isinstance(section, dict):  # Process the nested dictionary\n",
    "                        # Check for \"summary\" or \"report\" for \"Missing\"\n",
    "                        if \"summary\" in section and \"Missing\" in section[\"summary\"]:\n",
    "                            missing_count += 1\n",
    "                        if \"report\" in section:\n",
    "                            for item in section[\"report\"]:\n",
    "                                if any(\"Missing\" in retrieved for retrieved in item.get(\"retrieved\", [])):\n",
    "                                    missing_count += 1\n",
    "                                \n",
    "                        # Process the scores in \"decomp_score\"\n",
    "                        decomp_scores = section.get(\"decomp_score\", [])\n",
    "                        for score in decomp_scores:\n",
    "                            for key, value in score.items():\n",
    "                                if key in total_scores1:\n",
    "                                    total_scores1[key] += float(value)\n",
    "                                    count_scores1[key] += 1\n",
    "\n",
    "                        chat_scores = section.get(\"chat_eval\", [])\n",
    "                        for score in chat_scores:\n",
    "                            for key, value in score.items():\n",
    "                                if key in total_scores2:\n",
    "                                    total_scores2[key] += float(value)\n",
    "                                    count_scores2[key] += 1\n",
    "\n",
    "    # Calculate averages\n",
    "    average_scores1 = {key: (total_scores1[key] / count_scores1[key]) if count_scores1[key] > 0 else 0\n",
    "                      for key in total_scores1}\n",
    "    \n",
    "    average_scores2 = {key: (total_scores2[key] / count_scores2[key]) if count_scores2[key] > 0 else 0\n",
    "                      for key in total_scores2}\n",
    "    \n",
    "    # Print results\n",
    "    print(\"DecompEval Average Scores:\")\n",
    "    for key, avg in average_scores1.items():\n",
    "        print(f\"{key}: {avg:.2f}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"ChatEval Average Scores:\")\n",
    "    for key, avg in average_scores2.items():\n",
    "        print(f\"{key}: {avg:.2f}\")\n",
    "    \n",
    "    print(f\"\\nTotal 'Missing' count: {missing_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecompEval Average Scores:\n",
      "Financial takeaways: 0.28\n",
      "Financial context: 0.97\n",
      "Reasoning correctness: 0.99\n",
      "Management expectation: 0.63\n",
      "\n",
      "ChatEval Average Scores:\n",
      "Financial takeaways: 3.71\n",
      "Financial context: 3.99\n",
      "Reasoning correctness: 4.00\n",
      "Management expectation: 3.96\n",
      "\n",
      "Total 'Missing' count: 0\n"
     ]
    }
   ],
   "source": [
    "calculate_statistics(\"Result_Eval_decomposed_self_reflection\", \"INTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def post_process_markdown(text):\n",
    "    # Remove heading symbols (# and ##, etc.)\n",
    "    text = re.sub(r'#+\\s', '', text)\n",
    "\n",
    "    # Remove bullet points numbers (1., 2., etc.)\n",
    "    text = re.sub(r'\\d+\\.\\s', '', text)\n",
    "    \n",
    "    # Remove bold asterisks (**)\n",
    "    text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text)\n",
    "    \n",
    "    # Remove currency symbols, percentage signs, and similar formatting\n",
    "    text = re.sub(r'[\\$%]', '', text)\n",
    "    \n",
    "    # Remove extra line breaks and blank lines, merge paragraphs\n",
    "    text = re.sub(r'\\n+', '\\n', text).strip()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from readability import Readability\n",
    "\n",
    "import os\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def tradition_eval(text):\n",
    "    \"\"\"\n",
    "    Calculate readability scores for the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        dict: Readability scores including Flesch-Kincaid, Coleman-Liau, ARI, and SMOG.\n",
    "    \"\"\"\n",
    "    read = Readability(text)\n",
    "    scores = {\n",
    "        \"Flesch-Kincaid\": read.flesch_kincaid().score,\n",
    "        \"Coleman-Liau\": read.coleman_liau().score,\n",
    "        \"ARI\": read.ari().score\n",
    "    }\n",
    "    return scores\n",
    "\n",
    "def calculate_average_readability(result_path, company):\n",
    "    \"\"\"\n",
    "    Calculate the average readability scores, average sentence length, and sentence count per file \n",
    "    for all JSON files of a company.\n",
    "\n",
    "    Args:\n",
    "        company (str): The company name used to locate result files.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints average readability scores, average sentence length, and average sentences per file.\n",
    "    \"\"\"\n",
    "    # Initialize readability score accumulators and counters\n",
    "    readability_sums = {\n",
    "        \"Flesch-Kincaid\": 0,\n",
    "        \"Coleman-Liau\": 0,\n",
    "        \"ARI\": 0,\n",
    "        \"SMOG\": 0\n",
    "    }\n",
    "    total_sentence_count = 0\n",
    "    total_sentence_length = 0\n",
    "    file_count = 0\n",
    "\n",
    "    # List result files\n",
    "    result_files = os.listdir(f\"../{result_path}/{company}\")\n",
    "    \n",
    "    for result_file in result_files:\n",
    "        with open(f\"../../data/transcript/{result_file}\", 'r', encoding='utf-8') as f:\n",
    "            transcript = json.load(f)\n",
    "        with open(f\"../{result_path}/{company}/{result_file}\", 'r', encoding='utf-8') as f:\n",
    "            report_list = json.load(f)\n",
    "            merged_summary = \"\"\n",
    "\n",
    "            for report in report_list:\n",
    "                for key, section in report.items():\n",
    "                    if isinstance(section, dict):  # Process the nested dictionary\n",
    "                        # Merge summaries\n",
    "                        if \"summary\" in section:\n",
    "                            summary_text = section[\"summary\"]\n",
    "                            merged_summary += \" \" + summary_text.strip()\n",
    "\n",
    "            # Calculate readability score and sentence statistics for the merged summary\n",
    "            if merged_summary.strip():  # Ensure there is text to evaluate\n",
    "                merged_summary = post_process_markdown(merged_summary)\n",
    "                sentences = sent_tokenize(merged_summary)\n",
    "                sentence_count = len(sentences)\n",
    "                sentence_length = sum(len(word_tokenize(sentence)) for sentence in sentences)\n",
    "\n",
    "                total_sentence_count += sentence_count\n",
    "                total_sentence_length += sentence_length\n",
    "\n",
    "                readability_result = tradition_eval(merged_summary)\n",
    "                for key, value in readability_result.items():\n",
    "                    readability_sums[key] += value\n",
    "                file_count += 1\n",
    "\n",
    "    # Calculate and print average readability scores, sentence length, and sentences per file\n",
    "    if file_count > 0:\n",
    "        average_readability = {key: (value / file_count) for key, value in readability_sums.items()}\n",
    "        average_sentences_per_file = total_sentence_count / file_count\n",
    "\n",
    "        print(f\"\\nAverage Readability scores for {company}:\")\n",
    "        for key, value in average_readability.items():\n",
    "            print(f\"{key}: {value:.2f}\")\n",
    "        \n",
    "        print(f\"\\nAverage sentences per file for {company}: {average_sentences_per_file:.2f}\")\n",
    "    else:\n",
    "        print(f\"\\nNo valid summaries for readability in {company}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Readability scores for INTC:\n",
      "Flesch-Kincaid: 16.79\n",
      "Coleman-Liau: 17.19\n",
      "ARI: 18.37\n",
      "SMOG: 0.00\n",
      "\n",
      "Average sentences per file for INTC: 98.13\n"
     ]
    }
   ],
   "source": [
    "calculate_average_readability(\"Result_Eval_decomposed_self_reflection\", \"INTC\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
