{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def calculate_statistics(result_path, company):\n",
    "    # Initialize accumulators and counters, scores1 is for decomp_score, scores2 is for chat_eval\n",
    "    total_scores1 = {\n",
    "        \"Financial takeaways\": 0,\n",
    "        \"Financial context\": 0,\n",
    "        \"Reasoning correctness\": 0,\n",
    "        \"Management expectation\": 0\n",
    "    }\n",
    "    count_scores1 = {\n",
    "        \"Financial takeaways\": 0,\n",
    "        \"Financial context\": 0,\n",
    "        \"Reasoning correctness\": 0,\n",
    "        \"Management expectation\": 0\n",
    "    }\n",
    "    total_scores2 = {\n",
    "        \"Financial takeaways\": 0,\n",
    "        \"Financial context\": 0,\n",
    "        \"Reasoning correctness\": 0,\n",
    "        \"Management expectation\": 0\n",
    "    }\n",
    "    count_scores2 = {\n",
    "        \"Financial takeaways\": 0,\n",
    "        \"Financial context\": 0,\n",
    "        \"Reasoning correctness\": 0,\n",
    "        \"Management expectation\": 0\n",
    "    }\n",
    "    missing_count = 0\n",
    "\n",
    "    # List result files\n",
    "    result_files = os.listdir(f\"{result_path}/{company}\")\n",
    "    \n",
    "    for result_file in result_files:\n",
    "        with open(f\"{result_path}/{company}/{result_file}\", 'r', encoding='utf-8') as f:\n",
    "            report_list = json.load(f)\n",
    "            \n",
    "            miss = False\n",
    "            for report in report_list:\n",
    "                # Check for \"Missing\" in the sub-section content\n",
    "                for section, content in report.items():\n",
    "                    if isinstance(content, str) and \"Missing\" in content:\n",
    "                        missing_count += 1\n",
    "                        miss = True\n",
    "                if miss == True:\n",
    "                    continue\n",
    "                \n",
    "                # Process scores in decomp_score\n",
    "                decomp_score = report.get(\"decomp_score\", [])\n",
    "                for score in decomp_score:\n",
    "                    for key, value in score.items():\n",
    "                        if key in total_scores1:\n",
    "                            total_scores1[key] += float(value)\n",
    "                            count_scores1[key] += 1\n",
    "\n",
    "                chat_score = report.get(\"chat_eval\", [])\n",
    "                for score in chat_score:\n",
    "                    for key, value in score.items():\n",
    "                        if key in total_scores2:\n",
    "                            total_scores2[key] += float(value)\n",
    "                            count_scores2[key] += 1\n",
    "\n",
    "    # Calculate averages\n",
    "    average_scores1 = {key: (total_scores1[key] / count_scores1[key]) if count_scores1[key] > 0 else 0\n",
    "                      for key in total_scores1}\n",
    "    \n",
    "    average_scores2 = {key: (total_scores2[key] / count_scores2[key]) if count_scores2[key] > 0 else 0\n",
    "                      for key in total_scores2}\n",
    "    \n",
    "    # Print results\n",
    "    print(\"DecompEval Average Scores:\")\n",
    "    for key, avg in average_scores1.items():\n",
    "        print(f\"{key}: {avg:.2f}\")\n",
    "    print()\n",
    "    print(\"ChatEval Average Scores:\")\n",
    "    for key, avg in average_scores2.items():\n",
    "        print(f\"{key}: {avg:.2f}\")\n",
    "    \n",
    "    print(f\"\\nTotal 'Missing' count: {missing_count}\")\n",
    "\n",
    "    return average_scores1\n",
    "\n",
    "                        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecompEval Average Scores:\n",
      "Financial takeaways: 0.39\n",
      "Financial context: 0.89\n",
      "Reasoning correctness: 0.95\n",
      "Management expectation: 0.34\n",
      "\n",
      "ChatEval Average Scores:\n",
      "Financial takeaways: 3.36\n",
      "Financial context: 3.71\n",
      "Reasoning correctness: 3.86\n",
      "Management expectation: 3.19\n",
      "\n",
      "Total 'Missing' count: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Financial takeaways': 0.390344827586207,\n",
       " 'Financial context': 0.8910344827586208,\n",
       " 'Reasoning correctness': 0.9463793103448276,\n",
       " 'Management expectation': 0.3401724137931035}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_statistics(\"../Result/Result_Eval_self_reflection\", \"INTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from readability import Readability\n",
    "\n",
    "def tradition_eval(summary_text):\n",
    "    \"\"\"\n",
    "    Calculate readability scores for the given summary text.\n",
    "\n",
    "    Args:\n",
    "        summary_text (str): The summary text to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        dict: Readability scores including Flesch-Kincaid, Coleman-Liau, ARI, and SMOG.\n",
    "    \"\"\"\n",
    "    read = Readability(summary_text)\n",
    "    scores = {\n",
    "        \"Flesch-Kincaid\": read.flesch_kincaid().score,\n",
    "        \"Coleman-Liau\": read.coleman_liau().score,\n",
    "        \"ARI\": read.ari().score,\n",
    "    }\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def post_process_markdown(text):\n",
    "    # Remove heading symbols (# and ##, etc.)\n",
    "    text = re.sub(r'#+\\s', '', text)\n",
    "\n",
    "    # Remove bullet points numbers (1., 2., etc.)\n",
    "    text = re.sub(r'\\d+\\.\\s', '', text)\n",
    "    \n",
    "    # Remove bold asterisks (**)\n",
    "    text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text)\n",
    "    \n",
    "    # Remove currency symbols, percentage signs, and similar formatting\n",
    "    text = re.sub(r'[\\$%]', '', text)\n",
    "    \n",
    "    # Remove extra line breaks and blank lines, merge paragraphs\n",
    "    text = re.sub(r'\\n+', '\\n', text).strip()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def calculate_average_readability_and_sentences_per_file(result_path, company):\n",
    "    \"\"\"\n",
    "    Calculate the average readability scores and average sentence count per file for all JSON files of a company.\n",
    "\n",
    "    Args:\n",
    "        company (str): The company name used to locate result files.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints average readability scores and average sentence count per file for the company.\n",
    "    \"\"\"\n",
    "    # Initialize readability score accumulators and file count\n",
    "    readability_sums = {\n",
    "        \"Flesch-Kincaid\": 0,\n",
    "        \"Coleman-Liau\": 0,\n",
    "        \"ARI\": 0,\n",
    "    }\n",
    "    total_sentence_count = 0\n",
    "    file_count = 0\n",
    "\n",
    "    # List result files\n",
    "    result_files = os.listdir(f\"../{result_path}/{company}\")\n",
    "    \n",
    "    for result_file in result_files:\n",
    "        with open(f\"../../data/transcript/{result_file}\", 'r', encoding='utf-8') as f:\n",
    "            transcript = json.load(f)\n",
    "        with open(f\"../{result_path}/{company}/{result_file}\", 'r', encoding='utf-8') as f:\n",
    "            report_list = json.load(f)\n",
    "            merged_summary = \"\"\n",
    "\n",
    "            for report in report_list:\n",
    "                for key, value in report.items():\n",
    "                    if isinstance(value, str):  # Extract text from subsection (e.g., \"sub section 1.1\")\n",
    "                        merged_summary += \" \" + value.strip()\n",
    "\n",
    "            # Calculate readability score and sentence count for the merged summary\n",
    "            if merged_summary.strip():  # Ensure there is text to evaluate\n",
    "                merged_summary = post_process_markdown(merged_summary)\n",
    "                sentences = sent_tokenize(merged_summary)\n",
    "                total_sentence_count += len(sentences)\n",
    "\n",
    "                try:\n",
    "                    readability_result = tradition_eval(merged_summary)\n",
    "                except:\n",
    "                    readability_result = {\n",
    "                        \"Flesch-Kincaid\": 0,\n",
    "                        \"Coleman-Liau\": 0,\n",
    "                        \"ARI\": 0\n",
    "                    }\n",
    "                    file_count -= 1\n",
    "                for key, value in readability_result.items():\n",
    "                    readability_sums[key] += value\n",
    "                file_count += 1\n",
    "\n",
    "    # Calculate and print average readability scores and average sentence count per file\n",
    "    if file_count > 0:\n",
    "        average_readability = {key: (value / file_count) for key, value in readability_sums.items()}\n",
    "        average_sentences_per_file = total_sentence_count / file_count\n",
    "\n",
    "        print(f\"\\nAverage Readability scores for {company}:\")\n",
    "        for key, value in average_readability.items():\n",
    "            print(f\"{key}: {value:.2f}\")\n",
    "        \n",
    "        print(f\"\\nAverage sentences per file for {company}: {average_sentences_per_file:.2f}\")\n",
    "    else:\n",
    "        print(f\"\\nNo valid summaries for readability in {company}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Readability scores for INTC:\n",
      "Flesch-Kincaid: 15.70\n",
      "Coleman-Liau: 15.63\n",
      "ARI: 17.06\n",
      "\n",
      "Average sentences per file for INTC: 9.33\n"
     ]
    }
   ],
   "source": [
    "calculate_average_readability_and_sentences_per_file(\"Result/Result_Eval_self_reflection\", \"INTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
